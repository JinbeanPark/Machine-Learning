# -*- coding: utf-8 -*-
"""Fall2021-CS146-HW2.ipynb (Jinbean Park - 805330751)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ti--3e37mmolWlZddkxLDZKutw28FQ6a
"""

# This code was adapted from course material by Jenna Wiens (UMichigan).

"""
Name: Jinbean Park
UID: 805330751
Homework 2
"""

# python libraries
import os

# numpy libraries
import numpy as np

# matplotlib libraries
import matplotlib.pyplot as plt

# time libraries
import time

# To add your own Drive Run this cell.
from google.colab import drive
drive.mount('/content/drive')

######################################################################
# classes
######################################################################


class Data:
    def __init__(self, X=None, y=None):
        """
        Data class.
        
        Attributes
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
        """

        # n = number of examples, d = dimensionality
        self.X = X
        self.y = y

    def load(self, filename):
        """
        Load csv file into X array of features and y array of labels.
        
        Parameters
        --------------------
            filename -- string, filename
        """

        # load data
        with open(filename, "r") as fid:
            data = np.loadtxt(fid, delimiter=",")

        # separate features and labels
        self.X = data[:, :-1]
        self.y = data[:, -1]

    def plot(self, **kwargs):
        """Plot data."""

        if "color" not in kwargs:
            kwargs["color"] = "b"

        plt.scatter(self.X, self.y, **kwargs)
        plt.xlabel("x", fontsize=16)
        plt.ylabel("y", fontsize=16)
        plt.show()

# wrapper functions around Data class
def load_data(filename):
    data = Data()
    data.load(filename)
    return data


def plot_data(X, y, **kwargs):
    data = Data(X, y)
    data.plot(**kwargs)

class PolynomialRegression:
    def __init__(self, m=1, reg_param=0):
        """
        Ordinary least squares regression.
        
        Attributes
        --------------------
            coef_   -- numpy array of shape (d,)
                       estimated coefficients for the linear regression problem
            m_      -- integer
                       order for polynomial regression
            lambda_ -- float
                       regularization parameter
        """
        self.coef_ = None
        self.m_ = m
        self.lambda_ = reg_param

    def generate_polynomial_features(self, X):
        """
        Maps X to an mth degree feature vector e.g. [1, X, X^2, ..., X^m].
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,1), features
        
        Returns
        --------------------
            Phi     -- numpy array of shape (n,(m+1)), mapped features
        """

        n, d = X.shape

        ### ========== TODO : START ========== ###
        # part b: modify to create matrix for simple linear model
        # Check out whether self.m_ == 0.
        if self.m_ == 0:
          firstCol = np.ones_like(X)
          return firstCol
        # Check out whether X already has the proper column size or not.
        if np.shape(X)[1] == self.m_ + 1:
          return X
        # Creating the column having elements only 1.
        firstCol = np.ones_like(X)
        # Combine the firstCol with X.
        Phi = np.hstack((firstCol, X))
        #print(Phi)

        # part g: modify to create matrix for polynomial model
        # Create an m + 1 dimensional feature vector for each instance.
        for j in range(2, self.m_ + 1):
          Phi = np.hstack((Phi, X ** j))
        #print(Phi)
        ### ========== TODO : END ========== ###

        return Phi

    def fit_GD(self, X, y, eta=None, eps=0, tmax=10000, verbose=False):
        """
        Finds the coefficients of a {d-1}^th degree polynomial
        that fits the data using least squares batch gradient descent.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
            eta     -- float, step size
            eps     -- float, convergence criterion
            tmax    -- integer, maximum number of iterations
            verbose -- boolean, for debugging purposes
        
        Returns
        --------------------
            self    -- an instance of self
        """
        start = time.time()
        if self.lambda_ != 0:
            raise Exception("GD with regularization not implemented")

        X = self.generate_polynomial_features(X)  # map features
        n, d = X.shape
        eta_input = eta
        self.coef_ = np.zeros(d)  # coefficients
        err_list = np.zeros((tmax, 1))  # errors per iteration

        # GD loop
        for t in range(tmax):
            ### ========== TODO : START ========== ###
            # part f: update step size
            # change the default eta in the function signature to 'eta=None'
            # and update the line below to your learning rate function
            if eta_input is None:
              eta = (1.0 /  (1.0 + float(t)))
            ### ========== TODO : END ========== ###

            ### ========== TODO : START ========== ###
            # part d: update w (self.coef_) using one step of GD
            # hint: you can write simultaneously update all w using vector math
            # Calculates error.
            err = self.predict(X) - y
            # Calculates gardient by using the matrix multiplication.
            gradi = np.dot(err.T, X)
            # Update the weight.
            self.coef_ = self.coef_ - 2 * eta * gradi

            # track error
            # hint: you cannot use self.predict(...) to make the predictions
            # We don't have to update y, so we should not use self.predict(...)
            # Calculate the predicted value.
            predictedY = np.dot(X, self.coef_)
            # Compute the error by using the J function.
            err_list[t] = np.sum(np.power(predictedY - y, 2)) / float(n)
            
            ### ========== TODO : END ========== ###

            # stop?
            if t > 0 and abs(err_list[t] - err_list[t - 1]) <= eps:
                break

            # debugging
            # debugging
            if verbose :
                x = np.reshape(X[:,1], (n,1))
                cost = self.cost(x,y)
                #print ("iteration: %d, cost: %f" % (t+1, cost))
        end = time.time()

        # Print the value eta, iterations, elapsed time, cost, and weight.
        print("{:>30}".format(eta), "{:>30}".format(t + 1), "{:>30}".format(end - start), "{:>30}".format(self.cost(X,y)), "{:10}".format(""), self.coef_)
    
        #print("number of iterations: %d" % (t + 1))

        return self

    def fit(self, X, y, l2regularize=None):
        """
        Finds the coefficients of a {d-1}^th degree polynomial
        that fits the data using the closed form solution.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
            l2regularize    -- set to None for no regularization. set to positive double for L2 regularization
                
        Returns
        --------------------        
            self    -- an instance of self
        """

        X = self.generate_polynomial_features(X)  # map features

        #print("m: ", self.m_)

        ### ========== TODO : START ========== ###
        # part e: implement closed-form solution
        # hint: use np.dot(...) and np.linalg.pinv(...)
        # be sure to update self.coef_ with your solution
        # Calculate the running time of closed_form solution
        start = time.time()
        # Compute weight by using the linear regression formula for the closed-form solution.
        self.coef_ = np.linalg.pinv(np.dot(X.T, X)).dot(X.T).dot(y)
        end = time.time()

        print("Coefficients for closed_form solution: ", self.coef_)
        print("Cost for closed_form solution: ", self.cost(X, y))
        print("Running time for closed_form solution: ", (end - start))
        ### ========== TODO : END ========== ###

    def predict(self, X):
        """
        Predict output for X.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
        
        Returns
        --------------------
            y       -- numpy array of shape (n,), predictions
        """
        if self.coef_ is None:
            raise Exception("Model not initialized. Perform a fit first.")

        X = self.generate_polynomial_features(X)  # map features

        ### ========== TODO : START ========== ###
        # part c: predict y
        # Predict Y by multiplying X and wegihts.
        y = np.dot(X, self.coef_)

        #print("@@@@ predict y @@@@: ", y)
        ### ========== TODO : END ========== ###

        return y

    def cost(self, X, y):
        """
        Calculates the objective function.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
        
        Returns
        --------------------
            cost    -- float, objective J(w)
        """
        ### ========== TODO : START ========== ###
        # part d: compute J(w)
        predictedY = self.predict(X)
        # Compute square deviation.
        squDev = (predictedY - y) ** 2
        # Summation of square deviations.
        cost = np.sum(squDev)
        ### ========== TODO : END ========== ###
        return cost

    def rms_error(self, X, y):
        """
        Calculates the root mean square error.
        
        Parameters
        --------------------
            X       -- numpy array of shape (n,d), features
            y       -- numpy array of shape (n,), targets
        
        Returns
        --------------------
            error   -- float, RMSE
        """
        ### ========== TODO : START ========== ###
        # part h: compute RMSE
        N = X.shape[0]
        J = self.cost(self.generate_polynomial_features(X), y)
        error = (J / N) ** (1 / 2)
        ### ========== TODO : END ========== ###
        return error

    def plot_regression(self, xmin=0, xmax=1, n=50, **kwargs):
        """Plot regression line."""
        if "color" not in kwargs:
            kwargs["color"] = "r"
        if "linestyle" not in kwargs:
            kwargs["linestyle"] = "-"

        X = np.reshape(np.linspace(0, 1, n), (n, 1))
        y = self.predict(X)
        plot_data(X, y, **kwargs)
        plt.show()

######################################################################
# main
######################################################################

def main():
    # load data with correct file path

    data_directory_path =  "/content/drive/My Drive/CSM146"

    train_data = load_data(os.path.join(data_directory_path, "train.csv"))
    test_data = load_data(os.path.join(data_directory_path,"test.csv"))

    ### ========== TODO : START ========== ###
    # part a: main code for visualizations
    plot_data(train_data.X, train_data.y)
    plot_data(test_data.X, test_data.y)

    ### ========== TODO : END ========== ###

    print("Investigating linear regression...")

    ### ========== TODO : START ========== ###
    # parts b-f: main code for linear regression
    model = PolynomialRegression(1)
    model.coef_ = np.zeros(2)
    c = model.cost (train_data.X, train_data.y)
    print(f'model_cost:{c}')

    # For running GD
    learnRates = [0.0000001, 0.00001, 0.001, 0.0168]
    print("{:>30}".format("Learning rate"), "{:>30}".format("iterations"), "{:>30}".format("Time elapsed"), "{:>30}".format("Final value"), "{:>30}".format("Coefficients"))
    for learnRate in learnRates:
      model = PolynomialRegression(1)
      model.fit_GD(train_data.X, train_data.y, learnRate)
    
    # For running closed_form solution
    model = PolynomialRegression(1)
    model.fit(train_data.X, train_data.y)
    
    # For running GD with learning rate for the number of iterations.
    model = PolynomialRegression(1)
    print("{:>30}".format("Learning rate"), "{:>30}".format("iterations"), "{:>30}".format("Time elapsed"), "{:>30}".format("Final value"), "{:>30}".format("Coefficients"))
    model.fit_GD(train_data.X, train_data.y)

    
    ### ========== TODO : END ========== ###

    # parts g-i: main code for polynomial regression
    print ('Investigating polynomial regression...')
    degree = range (11)
    rmse_train = []; rmse_test = []
    bestM = None

    for m in degree:
        model = PolynomialRegression (m)
        model.fit(train_data.X, train_data.y)
        rmse_train.append( model.rms_error (train_data.X, train_data.y) )
        rmse_test.append( model.rms_error (test_data.X, test_data.y) )
    print(rmse_test)
    plt.figure ()
    plt.plot ( degree, rmse_train, 'bo-',label="train")
    plt.plot ( degree, rmse_test, 'ro-', label="test")
    plt.xlabel ('Degree'); plt.ylabel ('RMSE')
    plt.xlim (0,10)
    plt.legend(loc="upper left")
    plt.show ()

    minRMSE = min(rmse_test)
    print("Best degree: ", rmse_test.index(minRMSE), ", RMSE for best degree: ", minRMSE)

    print("Done!")

if __name__ == "__main__":
    main()